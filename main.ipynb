{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf07c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ https://vnexpress.net/rss/thoi-su.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/phap-luat.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/the-gioi.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/tin-moi-nhat.rss -> 53\n",
      "‚úÖ https://vnexpress.net/rss/suc-khoe.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/the-thao.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/giao-duc.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/doi-song.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/kinh-doanh.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/so-hoa.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/giai-tri.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/khoa-hoc.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/oto-xe-may.rss -> 60\n",
      "‚úÖ https://vnexpress.net/rss/y-kien.rss -> 60\n",
      "‚úÖ https://tuoitre.vn/rss/thoi-su.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/tin-moi.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/the-gioi.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/kinh-doanh.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/van-hoa.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/giao-duc.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/suc-khoe.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/the-thao.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/cong-nghe.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/phap-luat.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/nhip-song.rss -> 50\n",
      "‚úÖ https://tuoitre.vn/rss/xe.rss -> 50\n",
      "‚úÖ https://thanhnien.vn/rss/home.rss -> 60\n",
      "‚úÖ https://thanhnien.vn/rss/tai-chinh-kinh-doanh.rss -> 0\n",
      "‚úÖ https://thanhnien.vn/rss/thoi-su.rss -> 300\n",
      "‚úÖ https://dantri.com.vn/rss/xa-hoi.rss -> 100\n",
      "‚úÖ https://dantri.com.vn/rss/home.rss -> 100\n",
      "‚úÖ https://thanhnien.vn/rss/the-gioi.rss -> 300\n",
      "‚úÖ https://thanhnien.vn/rss/doi-song.rss -> 300\n",
      "‚úÖ https://dantri.com.vn/rss/giao-duc-khuyen-hoc.rss -> 0\n",
      "‚úÖ https://dantri.com.vn/rss/the-thao.rss -> 100\n",
      "‚úÖ https://dantri.com.vn/rss/phap-luat.rss -> 0\n",
      "‚úÖ https://dantri.com.vn/rss/kinh-doanh.rss -> 100\n",
      "‚úÖ https://dantri.com.vn/rss/the-gioi.rss -> 100\n",
      "‚úÖ https://thanhnien.vn/rss/van-hoa.rss -> 300‚úÖ https://dantri.com.vn/rss/van-hoa.rss -> 100\n",
      "\n",
      "‚úÖ https://dantri.com.vn/rss/oto-xe-may.rss -> 0\n",
      "‚úÖ https://thanhnien.vn/rss/the-thao.rss -> 300\n",
      "‚úÖ https://thanhnien.vn/rss/cong-nghe.rss -> 300\n",
      "‚úÖ https://dantri.com.vn/rss/suc-khoe.rss -> 100\n",
      "‚úÖ https://vietnamnet.vn/rss/tin-moi-nong.rss -> 0\n",
      "‚úÖ https://vietnamnet.vn/rss/thoi-su.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/the-gioi.rss -> 60\n",
      "‚úÖ https://thanhnien.vn/rss/xe.rss -> 300\n",
      "‚úÖ https://thanhnien.vn/rss/giao-duc.rss -> 300\n",
      "‚úÖ https://vietnamnet.vn/rss/kinh-doanh.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/giai-tri.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/the-thao.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/giao-duc.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/suc-khoe.rss -> 60\n",
      "‚úÖ https://vietnamnet.vn/rss/cong-nghe.rss -> 36\n",
      "‚úÖ https://zingnews.vn/rss/tin-moi.rss -> 0\n",
      "‚úÖ https://zingnews.vn/rss/the-gioi.rss -> 28\n",
      "‚úÖ https://zingnews.vn/rss/thoi-su.rss -> 50\n",
      "‚úÖ https://zingnews.vn/rss/the-thao.rss -> 50\n",
      "‚úÖ https://zingnews.vn/rss/kinh-doanh-tai-chinh.rss -> 50\n",
      "‚úÖ https://zingnews.vn/rss/giai-tri.rss -> 42\n",
      "‚úÖ https://zingnews.vn/rss/giao-duc.rss -> 16\n",
      "‚úÖ https://zingnews.vn/rss/cong-nghe.rss -> 10\n",
      "Total after dedup: 5028\n",
      "üìÅ Saved -> real.csv ( 5028 rows )\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd, re, time, hashlib, math, json\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -----------------------------\n",
    "# 1. RSS FEEDS\n",
    "# -----------------------------\n",
    "RSS_FEEDS = [\n",
    "    \"https://vnexpress.net/rss/tin-moi-nhat.rss\",\n",
    "    \"https://vnexpress.net/rss/thoi-su.rss\",\n",
    "    \"https://vnexpress.net/rss/the-gioi.rss\",\n",
    "    \"https://vnexpress.net/rss/kinh-doanh.rss\",\n",
    "    \"https://vnexpress.net/rss/giai-tri.rss\",\n",
    "    \"https://vnexpress.net/rss/the-thao.rss\",\n",
    "    \"https://vnexpress.net/rss/phap-luat.rss\",\n",
    "    \"https://vnexpress.net/rss/giao-duc.rss\",\n",
    "    \"https://vnexpress.net/rss/suc-khoe.rss\",\n",
    "    \"https://vnexpress.net/rss/doi-song.rss\",\n",
    "    \"https://vnexpress.net/rss/khoa-hoc.rss\",\n",
    "    \"https://vnexpress.net/rss/so-hoa.rss\",\n",
    "    \"https://vnexpress.net/rss/oto-xe-may.rss\",\n",
    "    \"https://vnexpress.net/rss/y-kien.rss\",\n",
    "    \"https://tuoitre.vn/rss/tin-moi.rss\",\n",
    "    \"https://tuoitre.vn/rss/thoi-su.rss\",\n",
    "    \"https://tuoitre.vn/rss/the-gioi.rss\",\n",
    "    \"https://tuoitre.vn/rss/kinh-doanh.rss\",\n",
    "    \"https://tuoitre.vn/rss/van-hoa.rss\",\n",
    "    \"https://tuoitre.vn/rss/the-thao.rss\",\n",
    "    \"https://tuoitre.vn/rss/phap-luat.rss\",\n",
    "    \"https://tuoitre.vn/rss/giao-duc.rss\",\n",
    "    \"https://tuoitre.vn/rss/suc-khoe.rss\",\n",
    "    \"https://tuoitre.vn/rss/nhip-song.rss\",\n",
    "    \"https://tuoitre.vn/rss/cong-nghe.rss\",\n",
    "    \"https://tuoitre.vn/rss/xe.rss\",\n",
    "    \"https://thanhnien.vn/rss/home.rss\",\n",
    "    \"https://thanhnien.vn/rss/thoi-su.rss\",\n",
    "    \"https://thanhnien.vn/rss/the-gioi.rss\",\n",
    "    \"https://thanhnien.vn/rss/tai-chinh-kinh-doanh.rss\",\n",
    "    \"https://thanhnien.vn/rss/doi-song.rss\",\n",
    "    \"https://thanhnien.vn/rss/van-hoa.rss\",\n",
    "    \"https://thanhnien.vn/rss/the-thao.rss\",\n",
    "    \"https://thanhnien.vn/rss/giao-duc.rss\",\n",
    "    \"https://thanhnien.vn/rss/cong-nghe.rss\",\n",
    "    \"https://thanhnien.vn/rss/xe.rss\",\n",
    "    \"https://dantri.com.vn/rss/home.rss\",\n",
    "    \"https://dantri.com.vn/rss/xa-hoi.rss\",\n",
    "    \"https://dantri.com.vn/rss/the-gioi.rss\",\n",
    "    \"https://dantri.com.vn/rss/kinh-doanh.rss\",\n",
    "    \"https://dantri.com.vn/rss/the-thao.rss\",\n",
    "    \"https://dantri.com.vn/rss/giao-duc-khuyen-hoc.rss\",\n",
    "    \"https://dantri.com.vn/rss/van-hoa.rss\",\n",
    "    \"https://dantri.com.vn/rss/phap-luat.rss\",\n",
    "    \"https://dantri.com.vn/rss/suc-khoe.rss\",\n",
    "    \"https://dantri.com.vn/rss/oto-xe-may.rss\",\n",
    "    \"https://vietnamnet.vn/rss/tin-moi-nong.rss\",\n",
    "    \"https://vietnamnet.vn/rss/thoi-su.rss\",\n",
    "    \"https://vietnamnet.vn/rss/the-gioi.rss\",\n",
    "    \"https://vietnamnet.vn/rss/kinh-doanh.rss\",\n",
    "    \"https://vietnamnet.vn/rss/giai-tri.rss\",\n",
    "    \"https://vietnamnet.vn/rss/the-thao.rss\",\n",
    "    \"https://vietnamnet.vn/rss/giao-duc.rss\",\n",
    "    \"https://vietnamnet.vn/rss/suc-khoe.rss\",\n",
    "    \"https://vietnamnet.vn/rss/cong-nghe.rss\",\n",
    "    \"https://zingnews.vn/rss/tin-moi.rss\",\n",
    "    \"https://zingnews.vn/rss/the-gioi.rss\",\n",
    "    \"https://zingnews.vn/rss/thoi-su.rss\",\n",
    "    \"https://zingnews.vn/rss/kinh-doanh-tai-chinh.rss\",\n",
    "    \"https://zingnews.vn/rss/the-thao.rss\",\n",
    "    \"https://zingnews.vn/rss/giai-tri.rss\",\n",
    "    \"https://zingnews.vn/rss/giao-duc.rss\",\n",
    "    \"https://zingnews.vn/rss/cong-nghe.rss\",\n",
    "]\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; FakeNewsVN/1.0)\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Helper functions\n",
    "# -----------------------------\n",
    "def clean_html(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"&[a-z]+;\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def crawl_feed(url, limit=300):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=20)\n",
    "        r.encoding = \"utf-8\"\n",
    "        soup = BeautifulSoup(r.text, \"xml\")\n",
    "        items = soup.find_all(\"item\")[:limit]\n",
    "        out = []\n",
    "        for it in items:\n",
    "            title = clean_html(it.title.text if it.title else \"\")\n",
    "            desc  = clean_html(it.description.text if it.description else \"\")\n",
    "            link  = (it.link.text if it.link else \"\").strip()\n",
    "            pub   = (it.pubDate.text if it.pubDate else \"\").strip()\n",
    "            out.append({\"title\": title, \"desc\": desc, \"link\": link, \"pubDate\": pub, \"source\": url})\n",
    "        print(f\"‚úÖ {url} -> {len(out)}\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {url} error: {e}\")\n",
    "        return []\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Crawl multi-thread\n",
    "# -----------------------------\n",
    "rows = []\n",
    "with ThreadPoolExecutor(max_workers=12) as ex:\n",
    "    futs = [ex.submit(crawl_feed, u, 300) for u in RSS_FEEDS]\n",
    "    for f in as_completed(futs):\n",
    "        rows.extend(f.result())\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# merge text\n",
    "df[\"text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"desc\"].fillna(\"\")).str.strip()\n",
    "\n",
    "# deduplicate\n",
    "key = (df[\"title\"].fillna(\"\") + \"|\" + df[\"link\"].fillna(\"\")).apply(lambda s: hashlib.md5(s.encode(\"utf-8\")).hexdigest())\n",
    "df[\"key\"] = key\n",
    "df = df.drop_duplicates(subset=[\"key\"]).drop(columns=[\"key\"])\n",
    "\n",
    "# convert pubDate -> datetime\n",
    "df[\"pubDate\"] = pd.to_datetime(df[\"pubDate\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "print(\"Total after dedup:\", len(df))\n",
    "df[\"label\"] = \"REAL\"\n",
    "df.to_csv(\"real.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"üìÅ Saved -> real.csv (\", len(df), \"rows )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c8b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\Desktop\\Demo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [00:11<00:00, 14.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re, math, time, tldextract, json\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, gradio as gr\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load & chu·∫©n ho√° d·ªØ li·ªáu\n",
    "# ----------------------------\n",
    "DF_PATH = \"real.csv\"\n",
    "\n",
    "def load_dataframe(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if \"text\" not in df.columns:\n",
    "        df[\"title\"] = df.get(\"title\",\"\").fillna(\"\")\n",
    "        df[\"desc\"]  = df.get(\"desc\",\"\").fillna(\"\")\n",
    "        df[\"text\"]  = (df[\"title\"] + \" \" + df[\"desc\"]).str.strip()\n",
    "    # parse th·ªùi gian\n",
    "    def parse_date(x):\n",
    "        try:\n",
    "            return pd.to_datetime(x, utc=True, errors=\"coerce\")\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    df[\"pubDate\"] = df.get(\"pubDate\",\"\").astype(str)\n",
    "    df[\"dt\"] = df[\"pubDate\"].apply(parse_date)\n",
    "    df[\"dt\"] = df[\"dt\"].fillna(pd.Timestamp(1970,1,1, tz=\"UTC\"))\n",
    "    # domain\n",
    "    def get_domain(row):\n",
    "        link = str(row.get(\"link\",\"\") or \"\")\n",
    "        src  = str(row.get(\"source\",\"\") or \"\")\n",
    "        url  = link if link else src\n",
    "        if not url:\n",
    "            return \"\"\n",
    "        ext = tldextract.extract(url)\n",
    "        # domain.suffix (v√≠ d·ª• vnexpress.net)\n",
    "        return \".\".join([p for p in [ext.domain, ext.suffix] if p])\n",
    "    df[\"domain\"] = df.apply(get_domain, axis=1)\n",
    "    # chu·∫©n text\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2) Scorer: trust theo domain + recency\n",
    "# ---------------------------------------\n",
    "TRUST = {\n",
    "    \"vnexpress.net\": 1.2,\n",
    "    \"tuoitre.vn\": 1.2,\n",
    "    \"thanhnien.vn\": 1.15,\n",
    "    \"dantri.com.vn\": 1.15,\n",
    "    \"vietnamnet.vn\": 1.1,\n",
    "    \"zingnews.vn\": 1.1,\n",
    "}\n",
    "def trust_weight(domain: str) -> float:\n",
    "    return TRUST.get(domain, 1.0)\n",
    "\n",
    "def recency_weight(dt: pd.Timestamp, half_life_days=21.0) -> float:\n",
    "    now = datetime.now(timezone.utc)\n",
    "    age_days = max(0, (now - dt).days if pd.notna(dt) else 3650)\n",
    "    return math.exp(-age_days / half_life_days)  # 0..1\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Retriever + Models (BM25, Dense, Cross, NLI)\n",
    "# ------------------------------------------------\n",
    "class Pipeline:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.corpus: List[str] = self.df[\"text\"].tolist()\n",
    "\n",
    "        # BM25\n",
    "        tokenized = [doc.split() for doc in self.corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "        # Dense embedder\n",
    "        self.embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "        self.corpus_emb = self.embedder.encode(self.corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "        # Cross-encoder (optional)\n",
    "        self.reranker = None\n",
    "        try:\n",
    "            self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        except Exception:\n",
    "            self.reranker = None\n",
    "\n",
    "        # NLI model\n",
    "        self.NLI_MODEL = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tok = AutoTokenizer.from_pretrained(self.NLI_MODEL)\n",
    "        self.nli_model = AutoModelForSequenceClassification.from_pretrained(self.NLI_MODEL).to(self.device)\n",
    "        self.id2label = {0:\"entailment\", 1:\"neutral\", 2:\"contradiction\"}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def nli_scores(self, premise: str, hypothesis: str) -> Dict[str, float]:\n",
    "        inputs = self.tok(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        logits = self.nli_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]\n",
    "        return {\n",
    "            self.id2label[0]: float(probs[0]),\n",
    "            self.id2label[1]: float(probs[1]),\n",
    "            self.id2label[2]: float(probs[2]),\n",
    "        }\n",
    "\n",
    "    def verify_claim(\n",
    "        self,\n",
    "        claim: str,\n",
    "        k_bm25=25,\n",
    "        k_dense=25,\n",
    "        top_m=12,\n",
    "        need_agree_sources=2,\n",
    "        need_disagree_sources=2,\n",
    "    ) -> Dict[str, Any]:\n",
    "        df = self.df\n",
    "        corpus = self.corpus\n",
    "\n",
    "        # BM25\n",
    "        bm25_scores = self.bm25.get_scores(claim.split())\n",
    "        top_bm = np.argsort(bm25_scores)[::-1][:k_bm25]\n",
    "\n",
    "        # Dense\n",
    "        q_emb = self.embedder.encode(claim, convert_to_tensor=True)\n",
    "        dense_scores = util.cos_sim(q_emb, self.corpus_emb)[0].detach().cpu().numpy()\n",
    "        top_de = np.argsort(dense_scores)[::-1][:k_dense]\n",
    "\n",
    "        # h·ª£p nh·∫•t\n",
    "        cand_idxs = list(set(top_bm.tolist() + top_de.tolist()))\n",
    "\n",
    "        # Cross-encoder rerank (optional)\n",
    "        ce_map = {i: 0.0 for i in cand_idxs}\n",
    "        if self.reranker is not None:\n",
    "            pairs = [(claim, corpus[i]) for i in cand_idxs]\n",
    "            try:\n",
    "                ce_scores = self.reranker.predict(pairs)  # cao l√† t·ªët\n",
    "                ce_map = {i: float(s) for i, s in zip(cand_idxs, ce_scores)}\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # min-max\n",
    "        def minmax(arr):\n",
    "            arr = np.array(arr, dtype=\"float32\")\n",
    "            mn, mx = float(arr.min()), float(arr.max())\n",
    "            if mx - mn < 1e-9:\n",
    "                return np.zeros_like(arr)\n",
    "            return (arr - mn) / (mx - mn)\n",
    "\n",
    "        bm_norm = minmax([bm25_scores[i] for i in cand_idxs])\n",
    "        de_norm = minmax([dense_scores[i] for i in cand_idxs])\n",
    "        ce_norm = minmax([ce_map[i] for i in cand_idxs])\n",
    "\n",
    "        # recency + trust\n",
    "        recs, trusts = [], []\n",
    "        for i in cand_idxs:\n",
    "            recs.append(recency_weight(df.iloc[i][\"dt\"]))\n",
    "            trusts.append(trust_weight(df.iloc[i][\"domain\"]))\n",
    "        recs = np.array(recs, dtype=\"float32\")\n",
    "        trusts = np.array(trusts, dtype=\"float32\")\n",
    "\n",
    "        final_scores = 0.35*bm_norm + 0.35*de_norm + 0.20*ce_norm + 0.10*(recs*trusts)\n",
    "        order = np.argsort(final_scores)[::-1][:top_m]\n",
    "        ranked = [cand_idxs[idx] for idx in order]\n",
    "\n",
    "        # NLI + quy·∫øt ƒë·ªãnh\n",
    "        evidences = []\n",
    "        entail_by_domain, contra_by_domain = {}, {}\n",
    "        entail_sum, contra_sum = 0.0, 0.0\n",
    "\n",
    "        for i in ranked:\n",
    "            row = df.iloc[i]\n",
    "            ev_text = str(row[\"text\"])\n",
    "            nli = self.nli_scores(ev_text, claim)\n",
    "            d = row[\"domain\"]\n",
    "            if nli[\"entailment\"] > nli[\"contradiction\"]:\n",
    "                entail_by_domain[d] = max(entail_by_domain.get(d, 0.0), nli[\"entailment\"])\n",
    "                entail_sum += nli[\"entailment\"]\n",
    "            else:\n",
    "                contra_by_domain[d] = max(contra_by_domain.get(d, 0.0), nli[\"contradiction\"])\n",
    "                contra_sum += nli[\"contradiction\"]\n",
    "\n",
    "            evidences.append({\n",
    "                \"id\": f\"doc_{i}\",\n",
    "                \"evidence\": ev_text[:400] + (\"...\" if len(ev_text) > 400 else \"\"),\n",
    "                \"bm25\": float(bm25_scores[i]),\n",
    "                \"dense\": float(dense_scores[i]),\n",
    "                \"ce\": float(ce_map.get(i, 0.0)),\n",
    "                \"recency_w\": float(recency_weight(row[\"dt\"])),\n",
    "                \"trust_w\": float(trust_weight(row[\"domain\"])),\n",
    "                \"entail\": round(nli[\"entailment\"], 4),\n",
    "                \"neutral\": round(nli[\"neutral\"], 4),\n",
    "                \"contra\": round(nli[\"contradiction\"], 4),\n",
    "                \"domain\": row.get(\"domain\",\"\"),\n",
    "                \"source\": row.get(\"source\",\"\"),\n",
    "                \"pubDate\": str(row.get(\"pubDate\",\"\")),\n",
    "                \"link\": row.get(\"link\",\"\"),\n",
    "            })\n",
    "\n",
    "        agree_sources = sum(1 for _d,_v in entail_by_domain.items() if _v >= 0.6)\n",
    "        disagree_sources = sum(1 for _d,_v in contra_by_domain.items() if _v >= 0.6)\n",
    "\n",
    "        label, conf = \"UNSURE\", 0.5\n",
    "        # ===== Decision Logic (fix) =====\n",
    "        # ∆Øu ti√™n 1: N·∫øu c√≥ √≠t nh·∫•t N ngu·ªìn tin c·∫≠y (trust ‚â• 1.1) ƒë·ªìng thu·∫≠n m·∫°nh ‚Üí REAL\n",
    "        strong_agree = sum(\n",
    "        1 for ev in evidences\n",
    "        if ev.get(\"entail\", 0) >= 0.55 and ev.get(\"trust_w\", 1.0) >= 1.1\n",
    "        )\n",
    "\n",
    "        # ∆Øu ti√™n 2: N·∫øu c√≥ √≠t nh·∫•t N ngu·ªìn ph·ªß ƒë·ªãnh m·∫°nh ‚Üí FAKE\n",
    "        strong_disagree = sum(\n",
    "        1 for ev in evidences\n",
    "        if ev.get(\"contra\", 0) >= 0.7\n",
    "        )\n",
    "\n",
    "        if strong_agree >= need_agree_sources:\n",
    "            label = \"REAL\"\n",
    "            conf = 0.9\n",
    "        elif strong_disagree >= need_disagree_sources:\n",
    "            label = \"FAKE\"\n",
    "            conf = 0.9\n",
    "        elif entail_sum > contra_sum:\n",
    "            label = \"REAL\"\n",
    "            conf = entail_sum / (entail_sum + contra_sum + 1e-6)\n",
    "        elif contra_sum > entail_sum:\n",
    "            label = \"FAKE\"\n",
    "            conf = contra_sum / (entail_sum + contra_sum + 1e-6)\n",
    "        else:\n",
    "            label = \"UNSURE\"\n",
    "            conf = 0.5\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"claim\": claim,\n",
    "            \"label\": label,\n",
    "            \"confidence\": round(float(conf), 3),\n",
    "            \"evidences\": evidences,\n",
    "            \"stats\": {\n",
    "                \"agree_sources\": int(agree_sources),\n",
    "                \"disagree_sources\": int(disagree_sources),\n",
    "                \"entail_sum\": round(float(entail_sum),3),\n",
    "                \"contra_sum\": round(float(contra_sum),3),\n",
    "            }\n",
    "        }\n",
    "\n",
    "# --------------------------\n",
    "# 4) Kh·ªüi t·∫°o pipeline lazy\n",
    "# --------------------------\n",
    "PIPE = None\n",
    "\n",
    "def init_pipeline(csv_path: str):\n",
    "    global PIPE\n",
    "    df = load_dataframe(csv_path)\n",
    "    PIPE = Pipeline(df)\n",
    "    return f\"Loaded {len(df)} documents from {csv_path}. Corpus ready.\"\n",
    "\n",
    "# --------------------------\n",
    "# 5) Gradio UI callbacks\n",
    "# --------------------------\n",
    "def do_verify(claim, k_bm25, k_dense, top_m, need_agree, need_disagree):\n",
    "    if not claim or len(claim.strip()) == 0:\n",
    "        return \"Nh·∫≠p n·ªôi dung c·∫ßn ki·ªÉm ch·ª©ng.\", {}, pd.DataFrame()\n",
    "    if PIPE is None:\n",
    "        return \"Pipeline ch∆∞a s·∫µn s√†ng.\", {}, pd.DataFrame()\n",
    "    out = PIPE.verify_claim(\n",
    "        claim=claim.strip(),\n",
    "        k_bm25=int(k_bm25),\n",
    "        k_dense=int(k_dense),\n",
    "        top_m=int(top_m),\n",
    "        need_agree_sources=int(need_agree),\n",
    "        need_disagree_sources=int(need_disagree),\n",
    "    )\n",
    "    # hi·ªÉn th·ªã label\n",
    "    headline = f\"**K·∫øt lu·∫≠n:** {out['label']} ‚Äî **ƒë·ªô tin c·∫≠y:** {out['confidence']}\"\n",
    "    # stats ra JSON\n",
    "    stats = {\n",
    "        \"agree_sources\": out[\"stats\"][\"agree_sources\"],\n",
    "        \"disagree_sources\": out[\"stats\"][\"disagree_sources\"],\n",
    "        \"entail_sum\": out[\"stats\"][\"entail_sum\"],\n",
    "        \"contra_sum\": out[\"stats\"][\"contra_sum\"],\n",
    "    }\n",
    "    # b·∫£ng evidence\n",
    "    ev = pd.DataFrame(out[\"evidences\"], columns=[\n",
    "        \"id\",\"domain\",\"pubDate\",\"entail\",\"contra\",\"recency_w\",\"trust_w\",\n",
    "        \"bm25\",\"dense\",\"ce\",\"source\",\"link\",\"evidence\"\n",
    "    ])\n",
    "    return headline, stats, ev\n",
    "\n",
    "def reload_csv(file):\n",
    "    path = file.name if file is not None else DF_PATH\n",
    "    msg = init_pipeline(path)\n",
    "    return gr.update(value=msg)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Build Gradio app\n",
    "# --------------------------\n",
    "with gr.Blocks(title=\"Fake News Verifier\") as demo:\n",
    "    gr.Markdown(\"## üïµÔ∏è‚Äç‚ôÇÔ∏è AI Agent ki·ªÉm ch·ª©ng tin ‚Äì Verifier Demo\")\n",
    "\n",
    "    with gr.Row():\n",
    "        claim = gr.Textbox(label=\"Nh·∫≠p ph√°t bi·ªÉu / tin c·∫ßn ki·ªÉm ch·ª©ng\", lines=3, placeholder=\"V√≠ d·ª•: 'B·ªô GD-ƒêT c√¥ng b·ªë l·ªãch thi m·ªõi...'\")\n",
    "    with gr.Accordion(\"Thi·∫øt l·∫≠p n√¢ng cao\", open=False):\n",
    "        with gr.Row():\n",
    "            k_bm25 = gr.Slider(5, 100, value=25, step=1, label=\"k_bm25\")\n",
    "            k_dense = gr.Slider(5, 100, value=25, step=1, label=\"k_dense\")\n",
    "            top_m = gr.Slider(5, 30, value=12, step=1, label=\"top_m (evidence hi·ªÉn th·ªã)\")\n",
    "        with gr.Row():\n",
    "            need_agree = gr.Slider(1, 5, value=2, step=1, label=\"Ngu·ªìn ƒë·ªìng thu·∫≠n t·ªëi thi·ªÉu\")\n",
    "            need_disagree = gr.Slider(1, 5, value=2, step=1, label=\"Ngu·ªìn ph·∫£n b√°c t·ªëi thi·ªÉu\")\n",
    "\n",
    "    with gr.Row():\n",
    "        run_btn = gr.Button(\"üîé Verify\", variant=\"primary\")\n",
    "\n",
    "    result_md = gr.Markdown()\n",
    "    stats_json = gr.JSON(label=\"Th·ªëng k√™\")\n",
    "    ev_df = gr.Dataframe(label=\"B·∫±ng ch·ª©ng\", wrap=True)\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"### D·ªØ li·ªáu c∆° s·ªü\")\n",
    "    with gr.Row():\n",
    "        csv_file = gr.File(label=\"T·∫£i CSV m·ªõi (tu·ª≥ ch·ªçn). C·ªôt c·∫ßn c√≥: title/desc ho·∫∑c text, link/source, pubDate\", file_types=[\".csv\"])\n",
    "        load_btn = gr.Button(\"üì• N·∫°p d·ªØ li·ªáu\")\n",
    "\n",
    "    status = gr.Textbox(label=\"Tr·∫°ng th√°i\", interactive=False)\n",
    "\n",
    "    # events\n",
    "    run_btn.click(do_verify, [claim, k_bm25, k_dense, top_m, need_agree, need_disagree], [result_md, stats_json, ev_df])\n",
    "    load_btn.click(reload_csv, [csv_file], [status])\n",
    "\n",
    "    # init on launch\n",
    "    demo.load(lambda: init_pipeline(DF_PATH), inputs=None, outputs=status)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
